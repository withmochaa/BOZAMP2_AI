{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 137\u001b[0m\n\u001b[1;32m    134\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    135\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(label_mapping)\n\u001b[0;32m--> 137\u001b[0m images, labels, class_counts \u001b[38;5;241m=\u001b[39m load_images_and_labels(folder_path\u001b[38;5;241m=\u001b[39mfolder_path, label_mapping\u001b[38;5;241m=\u001b[39mlabel_mapping, img_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m))\n\u001b[1;32m    139\u001b[0m encoder \u001b[38;5;241m=\u001b[39m OneHotEncoder(sparse_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    140\u001b[0m labels_onehot \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mfit_transform(labels\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[0;32mIn[1], line 43\u001b[0m, in \u001b[0;36mload_images_and_labels\u001b[0;34m(folder_path, label_mapping, img_size)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_name\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m         image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(file_path)\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: Image at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m could not be loaded.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adamax, AdamW\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# 얼굴 인식 모델 로드\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "def is_valid_image(face_image):\n",
    "    aspect_ratio = float(face_image.shape[1]) / face_image.shape[0]\n",
    "    if aspect_ratio < 0.4 or aspect_ratio > 2.5:\n",
    "        return False\n",
    "    mean_color = np.mean(face_image)\n",
    "    if mean_color < 30 or mean_color > 225:\n",
    "        return False\n",
    "    if face_image.shape[0] < 100 or face_image.shape[1] < 100:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def load_images_and_labels(folder_path, label_mapping, img_size=(128, 128)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_counts = {class_name: 0 for class_name in label_mapping.keys()}\n",
    "\n",
    "    for subfolder_name in os.listdir(folder_path):\n",
    "        subfolder_path = os.path.join(folder_path, subfolder_name)\n",
    "\n",
    "        if os.path.isdir(subfolder_path) and subfolder_name in label_mapping:\n",
    "            label = label_mapping[subfolder_name]\n",
    "\n",
    "            for file_name in os.listdir(subfolder_path):\n",
    "                file_path = os.path.join(subfolder_path, file_name)\n",
    "\n",
    "                if file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    try:\n",
    "                        image = cv2.imread(file_path)\n",
    "                        if image is None:\n",
    "                            print(f\"Warning: Image at {file_path} could not be loaded.\")\n",
    "                            continue\n",
    "\n",
    "                        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "                        for (x, y, w, h) in faces:\n",
    "                            face_image = image[y:y+h, x:x+w]\n",
    "                            face_image = cv2.resize(face_image, img_size)\n",
    "\n",
    "                            if is_valid_image(face_image):\n",
    "                                images.append(face_image / 255.0)  \n",
    "                                labels.append(label)\n",
    "                                class_counts[subfolder_name] += 1\n",
    "                            break\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading image {file_path}: {e}\")\n",
    "\n",
    "    print(f\"Total images loaded: {len(images)}\")\n",
    "    print(f\"Total labels collected: {len(labels)}\")\n",
    "    \n",
    "    unique_classes = np.unique(labels)\n",
    "    print(f\"Total unique classes: {len(unique_classes)}\")\n",
    "    \n",
    "    for class_name, count in class_counts.items():\n",
    "        print(f\"Class '{class_name}' has {count} images.\")\n",
    "\n",
    "    return np.array(images), np.array(labels), class_counts\n",
    "\n",
    "def visualize_images(images, labels, label_mapping, num_images_per_class=5):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i in range(len(label_mapping)):\n",
    "        class_images = images[labels == i]\n",
    "        if len(class_images) > 0:\n",
    "            random_indices = np.random.choice(len(class_images), size=min(num_images_per_class, len(class_images)), replace=False)\n",
    "            for j in range(len(random_indices)):\n",
    "                plt.subplot(len(label_mapping), num_images_per_class, i * num_images_per_class + j + 1)\n",
    "                plt.imshow(class_images[random_indices[j]])\n",
    "                plt.axis('off')\n",
    "                plt.title(list(label_mapping.keys())[i])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def build_model(input_shape, num_classes):\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "    for layer in base_model.layers[:-4]:  \n",
    "        layer.trainable = False\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = base_model(inputs)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Dense(128, activation='elu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)  \n",
    "    x = layers.Dense(64, activation='elu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)  \n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def detect_faces(images):\n",
    "    detected_images = []\n",
    "    detected_labels = []\n",
    "    \n",
    "    for i, image in enumerate(images):\n",
    "        image_uint8 = (image * 255).astype(np.uint8)  \n",
    "        gray = cv2.cvtColor(image_uint8, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "        if len(faces) > 0:\n",
    "            detected_images.append(image)\n",
    "            detected_labels.append(i)  \n",
    "    return np.array(detected_images), np.array(detected_labels)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"/Users/withmocha/Desktop/DATA/BOAZ/미니 프로젝트 2/data/face data/\"\n",
    "    \n",
    "    label_mapping = {\n",
    "        \"spring\": 0,\n",
    "        \"summer\": 1,\n",
    "        \"fall\": 2,\n",
    "        \"winter\": 3\n",
    "    }\n",
    "\n",
    "    input_shape = (128, 128, 3)\n",
    "    num_classes = len(label_mapping)\n",
    "\n",
    "    images, labels, class_counts = load_images_and_labels(folder_path=folder_path, label_mapping=label_mapping, img_size=(128, 128))\n",
    "\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    labels_onehot = encoder.fit_transform(labels.reshape(-1, 1))\n",
    "\n",
    "    print(\"Counts of validated images per class:\")\n",
    "    for class_name, count in class_counts.items():\n",
    "        print(f\"Class '{class_name}': {count} images\")\n",
    "\n",
    "    visualize_images(images, labels, label_mapping, num_images_per_class=5)\n",
    "\n",
    "    detected_images, detected_labels = detect_faces(images)\n",
    "\n",
    "    detected_class_counts = {class_name: 0 for class_name in label_mapping.keys()}\n",
    "    \n",
    "    detected_labels_as_classes = []\n",
    "\n",
    "    for idx in detected_labels:\n",
    "        detected_label = labels[idx]\n",
    "        detected_labels_as_classes.append(detected_label)\n",
    "       \n",
    "    for detected_label in detected_labels_as_classes:\n",
    "        class_name = list(label_mapping.keys())[detected_label]  \n",
    "        detected_class_counts[class_name] += 1  \n",
    "\n",
    "    print(\"Counts of detected faces per class:\")\n",
    "    for class_name, count in detected_class_counts.items():\n",
    "        print(f\"Class '{class_name}': {count} images\")  \n",
    "\n",
    "    for class_name in detected_class_counts.keys():\n",
    "        if detected_class_counts[class_name] > 0:\n",
    "            class_images = detected_images[np.array(detected_labels_as_classes) == label_mapping[class_name]]\n",
    "            visualize_images(class_images, np.full(len(class_images), label_mapping[class_name]), label_mapping, num_images_per_class=5)\n",
    "\n",
    "    train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "        detected_images, detected_labels_as_classes, test_size=0.1, stratify=detected_labels_as_classes, shuffle=True\n",
    "    )\n",
    "\n",
    "    train_labels = np.array(train_labels)\n",
    "    val_labels = np.array(val_labels)\n",
    "\n",
    "    train_labels_onehot = encoder.fit_transform(train_labels.reshape(-1, 1))\n",
    "    val_labels_onehot = encoder.transform(val_labels.reshape(-1, 1))\n",
    "\n",
    "    model = build_model(input_shape=input_shape, num_classes=num_classes)\n",
    "\n",
    "    optimizer = AdamW(learning_rate=0.00005)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_images,\n",
    "        train_labels_onehot,  \n",
    "        validation_data=(val_images, val_labels_onehot),  \n",
    "        epochs=100,\n",
    "        callbacks=[early_stopping, reduce_lr]\n",
    "    )\n",
    "\n",
    "    val_predictions = model.predict(val_images)\n",
    "    val_predictions_classes = np.argmax(val_predictions, axis=-1)\n",
    "\n",
    "    val_labels_classes = np.argmax(val_labels_onehot, axis=1)  \n",
    "    print(classification_report(val_labels_classes, val_predictions_classes, target_names=list(label_mapping.keys())))\n",
    "\n",
    "    cm = confusion_matrix(val_labels_classes, val_predictions_classes)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(num_classes)\n",
    "    plt.xticks(tick_marks, list(label_mapping.keys()), rotation=45)\n",
    "    plt.yticks(tick_marks, list(label_mapping.keys()))\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
